{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC160 Data Science and the Arts - Twomey - Spring 2020 - [dsc160.roberttwomey.com](http://dsc160.roberttwomey.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "train_images = []\n",
    "size = 128, 128\n",
    "counter = 0\n",
    "for img in os.listdir('./data/superheroes'):\n",
    "    try:\n",
    "        path = os.path.join('./data/superheroes',img)\n",
    "        a = Image.open(path).convert(\"RGB\")\n",
    "        a = a.resize(size)\n",
    "        train_images.append(np.array(a))\n",
    "        a.save('images/resized_image'+str(counter)+'.jpg')\n",
    "        counter = counter + 1\n",
    "    except:\n",
    "        pass\n",
    "train_images = np.array(train_images)\n",
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network\n",
    "\n",
    "\n",
    "Adapted from https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "run one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -q imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import time\n",
    "from skimage import io\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  8,  11,  14, ..., 127, 124, 123],\n",
       "        [ 19,  12,   9, ..., 124, 122, 122],\n",
       "        [ 23,   9,   3, ..., 121, 121, 122],\n",
       "        ...,\n",
       "        [134,  83,  30, ..., 154, 144, 126],\n",
       "        [145,  59,  20, ..., 155, 154, 144],\n",
       "        [153,  41,  16, ..., 156, 161, 157]],\n",
       "\n",
       "       [[ 33,  47,  49, ...,  38,  43,  34],\n",
       "        [ 52,  97, 130, ..., 113, 105,  48],\n",
       "        [ 53, 134, 203, ..., 172, 144,  37],\n",
       "        ...,\n",
       "        [ 58, 134, 200, ..., 198, 132,  53],\n",
       "        [ 52, 103, 139, ..., 131,  98,  48],\n",
       "        [ 17,  47,  57, ...,  49,  44,  18]],\n",
       "\n",
       "       [[130, 130, 151, ..., 142, 137, 139],\n",
       "        [129, 130, 154, ..., 158, 147, 130],\n",
       "        [124, 130, 155, ..., 176, 142, 103],\n",
       "        ...,\n",
       "        [159, 155, 151, ..., 180, 179, 180],\n",
       "        [159, 153, 149, ..., 173, 170, 171],\n",
       "        [159, 154, 150, ..., 169, 167, 167]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [240, 242, 251, ..., 250, 248, 248],\n",
       "        [242, 244, 248, ..., 247, 248, 248],\n",
       "        [243, 244, 246, ..., 246, 248, 248]],\n",
       "\n",
       "       [[ 94,  92,  46, ...,   0,   0,   0],\n",
       "        [ 96,  97,  97, ...,   0,   0,   0],\n",
       "        [ 66,  78,  85, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 84,  84,  84, ...,   0,   0,   0],\n",
       "        [ 30,  28,  23, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[110, 108, 112, ..., 165, 163, 163],\n",
       "        [116, 110, 111, ..., 168, 169, 171],\n",
       "        [122, 112, 109, ..., 174, 178, 182],\n",
       "        ...,\n",
       "        [175, 187, 183, ...,  72,  79, 137],\n",
       "        [173, 191, 184, ...,  44,  94, 140],\n",
       "        [175, 191, 184, ...,  34, 108, 142]]], dtype=uint8)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_images =np.empty((256,256), int)\n",
    "train_images = []\n",
    "size = 256,256\n",
    "for img in os.listdir('./data/superheroes'):\n",
    "    try:\n",
    "        path = os.path.join('./data/superheroes',img)\n",
    "        a = Image.open(path).convert(\"L\")\n",
    "        a = a.resize(size)\n",
    "        train_images.append(np.array(a))\n",
    "    except:\n",
    "        pass\n",
    "train_images = np.array(train_images)\n",
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.96875   ],\n",
       "         [-0.95703125],\n",
       "         [-0.9453125 ],\n",
       "         ...,\n",
       "         [-0.50390625],\n",
       "         [-0.515625  ],\n",
       "         [-0.51953125]],\n",
       "\n",
       "        [[-0.92578125],\n",
       "         [-0.953125  ],\n",
       "         [-0.96484375],\n",
       "         ...,\n",
       "         [-0.515625  ],\n",
       "         [-0.5234375 ],\n",
       "         [-0.5234375 ]],\n",
       "\n",
       "        [[-0.91015625],\n",
       "         [-0.96484375],\n",
       "         [-0.98828125],\n",
       "         ...,\n",
       "         [-0.52734375],\n",
       "         [-0.52734375],\n",
       "         [-0.5234375 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4765625 ],\n",
       "         [-0.67578125],\n",
       "         [-0.8828125 ],\n",
       "         ...,\n",
       "         [-0.3984375 ],\n",
       "         [-0.4375    ],\n",
       "         [-0.5078125 ]],\n",
       "\n",
       "        [[-0.43359375],\n",
       "         [-0.76953125],\n",
       "         [-0.921875  ],\n",
       "         ...,\n",
       "         [-0.39453125],\n",
       "         [-0.3984375 ],\n",
       "         [-0.4375    ]],\n",
       "\n",
       "        [[-0.40234375],\n",
       "         [-0.83984375],\n",
       "         [-0.9375    ],\n",
       "         ...,\n",
       "         [-0.390625  ],\n",
       "         [-0.37109375],\n",
       "         [-0.38671875]]],\n",
       "\n",
       "\n",
       "       [[[-0.87109375],\n",
       "         [-0.81640625],\n",
       "         [-0.80859375],\n",
       "         ...,\n",
       "         [-0.8515625 ],\n",
       "         [-0.83203125],\n",
       "         [-0.8671875 ]],\n",
       "\n",
       "        [[-0.796875  ],\n",
       "         [-0.62109375],\n",
       "         [-0.4921875 ],\n",
       "         ...,\n",
       "         [-0.55859375],\n",
       "         [-0.58984375],\n",
       "         [-0.8125    ]],\n",
       "\n",
       "        [[-0.79296875],\n",
       "         [-0.4765625 ],\n",
       "         [-0.20703125],\n",
       "         ...,\n",
       "         [-0.328125  ],\n",
       "         [-0.4375    ],\n",
       "         [-0.85546875]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7734375 ],\n",
       "         [-0.4765625 ],\n",
       "         [-0.21875   ],\n",
       "         ...,\n",
       "         [-0.2265625 ],\n",
       "         [-0.484375  ],\n",
       "         [-0.79296875]],\n",
       "\n",
       "        [[-0.796875  ],\n",
       "         [-0.59765625],\n",
       "         [-0.45703125],\n",
       "         ...,\n",
       "         [-0.48828125],\n",
       "         [-0.6171875 ],\n",
       "         [-0.8125    ]],\n",
       "\n",
       "        [[-0.93359375],\n",
       "         [-0.81640625],\n",
       "         [-0.77734375],\n",
       "         ...,\n",
       "         [-0.80859375],\n",
       "         [-0.828125  ],\n",
       "         [-0.9296875 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.4921875 ],\n",
       "         [-0.4921875 ],\n",
       "         [-0.41015625],\n",
       "         ...,\n",
       "         [-0.4453125 ],\n",
       "         [-0.46484375],\n",
       "         [-0.45703125]],\n",
       "\n",
       "        [[-0.49609375],\n",
       "         [-0.4921875 ],\n",
       "         [-0.3984375 ],\n",
       "         ...,\n",
       "         [-0.3828125 ],\n",
       "         [-0.42578125],\n",
       "         [-0.4921875 ]],\n",
       "\n",
       "        [[-0.515625  ],\n",
       "         [-0.4921875 ],\n",
       "         [-0.39453125],\n",
       "         ...,\n",
       "         [-0.3125    ],\n",
       "         [-0.4453125 ],\n",
       "         [-0.59765625]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.37890625],\n",
       "         [-0.39453125],\n",
       "         [-0.41015625],\n",
       "         ...,\n",
       "         [-0.296875  ],\n",
       "         [-0.30078125],\n",
       "         [-0.296875  ]],\n",
       "\n",
       "        [[-0.37890625],\n",
       "         [-0.40234375],\n",
       "         [-0.41796875],\n",
       "         ...,\n",
       "         [-0.32421875],\n",
       "         [-0.3359375 ],\n",
       "         [-0.33203125]],\n",
       "\n",
       "        [[-0.37890625],\n",
       "         [-0.3984375 ],\n",
       "         [-0.4140625 ],\n",
       "         ...,\n",
       "         [-0.33984375],\n",
       "         [-0.34765625],\n",
       "         [-0.34765625]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         ...,\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625]],\n",
       "\n",
       "        [[-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         ...,\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625]],\n",
       "\n",
       "        [[-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         ...,\n",
       "         [-0.00390625],\n",
       "         [-0.00390625],\n",
       "         [-0.00390625]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0625    ],\n",
       "         [-0.0546875 ],\n",
       "         [-0.01953125],\n",
       "         ...,\n",
       "         [-0.0234375 ],\n",
       "         [-0.03125   ],\n",
       "         [-0.03125   ]],\n",
       "\n",
       "        [[-0.0546875 ],\n",
       "         [-0.046875  ],\n",
       "         [-0.03125   ],\n",
       "         ...,\n",
       "         [-0.03515625],\n",
       "         [-0.03125   ],\n",
       "         [-0.03125   ]],\n",
       "\n",
       "        [[-0.05078125],\n",
       "         [-0.046875  ],\n",
       "         [-0.0390625 ],\n",
       "         ...,\n",
       "         [-0.0390625 ],\n",
       "         [-0.03125   ],\n",
       "         [-0.03125   ]]],\n",
       "\n",
       "\n",
       "       [[[-0.6328125 ],\n",
       "         [-0.640625  ],\n",
       "         [-0.8203125 ],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]],\n",
       "\n",
       "        [[-0.625     ],\n",
       "         [-0.62109375],\n",
       "         [-0.62109375],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]],\n",
       "\n",
       "        [[-0.7421875 ],\n",
       "         [-0.6953125 ],\n",
       "         [-0.66796875],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.671875  ],\n",
       "         [-0.671875  ],\n",
       "         [-0.671875  ],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]],\n",
       "\n",
       "        [[-0.8828125 ],\n",
       "         [-0.890625  ],\n",
       "         [-0.91015625],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]],\n",
       "\n",
       "        [[-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         ...,\n",
       "         [-1.        ],\n",
       "         [-1.        ],\n",
       "         [-1.        ]]],\n",
       "\n",
       "\n",
       "       [[[-0.5703125 ],\n",
       "         [-0.578125  ],\n",
       "         [-0.5625    ],\n",
       "         ...,\n",
       "         [-0.35546875],\n",
       "         [-0.36328125],\n",
       "         [-0.36328125]],\n",
       "\n",
       "        [[-0.546875  ],\n",
       "         [-0.5703125 ],\n",
       "         [-0.56640625],\n",
       "         ...,\n",
       "         [-0.34375   ],\n",
       "         [-0.33984375],\n",
       "         [-0.33203125]],\n",
       "\n",
       "        [[-0.5234375 ],\n",
       "         [-0.5625    ],\n",
       "         [-0.57421875],\n",
       "         ...,\n",
       "         [-0.3203125 ],\n",
       "         [-0.3046875 ],\n",
       "         [-0.2890625 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.31640625],\n",
       "         [-0.26953125],\n",
       "         [-0.28515625],\n",
       "         ...,\n",
       "         [-0.71875   ],\n",
       "         [-0.69140625],\n",
       "         [-0.46484375]],\n",
       "\n",
       "        [[-0.32421875],\n",
       "         [-0.25390625],\n",
       "         [-0.28125   ],\n",
       "         ...,\n",
       "         [-0.828125  ],\n",
       "         [-0.6328125 ],\n",
       "         [-0.453125  ]],\n",
       "\n",
       "        [[-0.31640625],\n",
       "         [-0.25390625],\n",
       "         [-0.28125   ],\n",
       "         ...,\n",
       "         [-0.8671875 ],\n",
       "         [-0.578125  ],\n",
       "         [-0.4453125 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 256, 256, 1).astype('float32')\n",
    "train_images = (train_images - 256) / 256# Normalize the images to [-1, 1]\n",
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = train_images.shape[0]\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Create the models using the Keras sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "[more here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate an image with our (untrained!) generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16c3cbe10>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "CNN-based image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our (untrained!) discriminator to classify this output as real/fake (pos, neg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00090037]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "helper for cross-entropy loss (typical for binary classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Loss\n",
    "\n",
    "computes discriminator performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Loss\n",
    "\n",
    "quantifies how well the generator was able to fool the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "G and D each have separate optimizers since we are training them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "This section sets the basic training parameters and what our test output is (`num_examples_to_generate`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single training step:\n",
    "1. generate a batch of images from a batch of noise vector\n",
    "2. test the discriminator on the generated images and a set of real images\n",
    "3. compute the generator and disciminator losses.\n",
    "4. calculate gradients, and apply gradients (optimizer) to trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop: iterate for some number of epochs, completing one training step (one batch) for each epoch. Save a checkpoint every 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function to generate (and save) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./gan_outputs/'):\n",
    "     os.makedirs('./gan_outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('./gan_outputs/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call the training method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [100,524288], In[1]: [6272,1] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-d152560ca122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-209-802af7bf198a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-5e6169cd6a67>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    897\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    693\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    897\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [100,524288], In[1]: [6272,1] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a GIF from training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('./gan_outputs/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Checkpoint\n",
    "\n",
    "After training, restore the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- Try changing/extending the number of epochs: does your generated result improve? \n",
    "- Try with a different dataset. The easiest way might be to work with the same image dimensions (28x28, monochrome), so find some set you can transform to that dimension/channel count.\n",
    "  - for extension, change the architecture of the network to work with larger images, or RGB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Generative Adversarial Nets, Goodfellow et. al (2014) https://arxiv.org/pdf/1406.2661.pdf\n",
    "- NeurIPS 2016 Tutorial: Generative Adversarial Networks, Ian Goodfellow https://arxiv.org/pdf/1701.00160.pdf\n",
    "- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, Radford et al (2015) https://arxiv.org/pdf/1511.06434.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
